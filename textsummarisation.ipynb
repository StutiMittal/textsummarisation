{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRy16xtXWOle",
        "outputId": "f7dfec6c-ae97-4073-f48c-c74086dad976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "pip install gensim nltk scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "#from gensim.summarization import summarize as gensim_summarize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import numpy as np\n",
        "from gensim.summarization.summarizer import summarizer\n",
        "from gensim.summarization import keywords\n",
        "# Sample text\n",
        "original_text = \"\"\"\n",
        "The advancements in artificial intelligence have led to significant breakthroughs in various domains.\n",
        "Machine learning algorithms have become increasingly sophisticated, enabling computers to perform complex tasks\n",
        "with minimal human intervention. Natural Language Processing (NLP) is one such field where AI has made\n",
        "remarkable strides. NLP focuses on the interaction between computers and humans through natural language.\n",
        "This involves tasks such as language translation, sentiment analysis, and text summarization. Text summarization,\n",
        "in particular, is crucial for condensing large amounts of information into concise and informative summaries.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(original_text)\n",
        "\n",
        "# Gensim Extractive Summarization\n",
        "gensim_summary = gensim_summarize(original_text)\n",
        "\n",
        "# Rule-Based Summarization\n",
        "# Example: Select the first and last sentences\n",
        "rule_based_summary = sentences[0] + \" \" + sentences[-1]\n",
        "\n",
        "# Latent Semantic Analysis (LSA)\n",
        "def lsa_summary(text, num_topics=1):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform([text])\n",
        "\n",
        "    # Apply Latent Semantic Analysis\n",
        "    svd = TruncatedSVD(n_components=num_topics)\n",
        "    lsa_matrix = svd.fit_transform(X)\n",
        "\n",
        "    # Extract the most important sentence based on LSA\n",
        "    important_sentence_index = np.argmax(lsa_matrix)\n",
        "    lsa_summary = sentences[important_sentence_index]\n",
        "\n",
        "    return lsa_summary\n",
        "\n",
        "# Calculate TOPSIS scores (hypothetical)\n",
        "scores = {\n",
        "    \"gensim\": {\"Relevance\": 8, \"Coherence\": 7, \"Conciseness\": 9, \"Coverage\": 8},\n",
        "    \"rule_based\": {\"Relevance\": 6, \"Coherence\": 6, \"Conciseness\": 7, \"Coverage\": 6},\n",
        "    \"lsa\": {\"Relevance\": 7, \"Coherence\": 7, \"Conciseness\": 8, \"Coverage\": 7}\n",
        "}\n",
        "\n",
        "# Weights for each criterion\n",
        "weights = {\"Relevance\": 4, \"Coherence\": 3, \"Conciseness\": 2, \"Coverage\": 1}\n",
        "\n",
        "# Calculate weighted sums\n",
        "weighted_sums = {model: sum(scores[model][criterion] * weights[criterion] for criterion in weights) for model in scores}\n",
        "\n",
        "# Determine the best model\n",
        "best_model = max(weighted_sums, key=weighted_sums.get)\n",
        "\n",
        "# Display results\n",
        "print(\"Gensim Summary:\")\n",
        "print(gensim_summary)\n",
        "print(\"\\nRule-Based Summary:\")\n",
        "print(rule_based_summary)\n",
        "print(\"\\nLSA Summary:\")\n",
        "print(lsa_summary(original_text))\n",
        "print(\"\\nTOPSIS Weighted Sums:\")\n",
        "print(weighted_sums)\n",
        "print(\"\\nBest Model:\")\n",
        "print(best_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "MDTJWcA0WTC8",
        "outputId": "6290f226-d429-43b6-891a-6a26e986ee35"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim.summarization'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d94d61873c81>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Sample text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from gensim.summarization import summarize as gensim_summarize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import numpy as np\n",
        "\n",
        "# Sample text\n",
        "original_text = \"\"\"\n",
        "The advancements in artificial intelligence have led to significant breakthroughs in various domains.\n",
        "Machine learning algorithms have become increasingly sophisticated, enabling computers to perform complex tasks\n",
        "with minimal human intervention. Natural Language Processing (NLP) is one such field where AI has made\n",
        "remarkable strides. NLP focuses on the interaction between computers and humans through natural language.\n",
        "This involves tasks such as language translation, sentiment analysis, and text summarization. Text summarization,\n",
        "in particular, is crucial for condensing large amounts of information into concise and informative summaries.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(original_text)\n",
        "\n",
        "# Gensim Extractive Summarization\n",
        "gensim_summary = gensim_summarize(original_text)\n",
        "\n",
        "# Rule-Based Summarization\n",
        "# Example: Select the first and last sentences\n",
        "rule_based_summary = sentences[0] + \" \" + sentences[-1]\n",
        "\n",
        "# Latent Semantic Analysis (LSA)\n",
        "def lsa_summary(text, num_topics=1):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform([text])\n",
        "\n",
        "    # Apply Latent Semantic Analysis\n",
        "    svd = TruncatedSVD(n_components=num_topics)\n",
        "    lsa_matrix = svd.fit_transform(X)\n",
        "\n",
        "    # Extract the most important sentence based on LSA\n",
        "    important_sentence_index = np.argmax(lsa_matrix)\n",
        "    lsa_summary = sentences[important_sentence_index]\n",
        "\n",
        "    return lsa_summary\n",
        "\n",
        "# Calculate TOPSIS scores (hypothetical)\n",
        "scores = {\n",
        "    \"gensim\": {\"Relevance\": 8, \"Coherence\": 7, \"Conciseness\": 9, \"Coverage\": 8},\n",
        "    \"rule_based\": {\"Relevance\": 6, \"Coherence\": 6, \"Conciseness\": 7, \"Coverage\": 6},\n",
        "    \"lsa\": {\"Relevance\": 7, \"Coherence\": 7, \"Conciseness\": 8, \"Coverage\": 7}\n",
        "}\n",
        "\n",
        "# Weights for each criterion\n",
        "weights = {\"Relevance\": 4, \"Coherence\": 3, \"Conciseness\": 2, \"Coverage\": 1}\n",
        "\n",
        "# Calculate weighted sums\n",
        "weighted_sums = {model: sum(scores[model][criterion] * weights[criterion] for criterion in weights) for model in scores}\n",
        "\n",
        "# Determine the best model\n",
        "best_model = max(weighted_sums, key=weighted_sums.get)\n",
        "\n",
        "# Display results\n",
        "print(\"Gensim Summary:\")\n",
        "print(gensim_summary)\n",
        "print(\"\\nRule-Based Summary:\")\n",
        "print(rule_based_summary)\n",
        "print(\"\\nLSA Summary:\")\n",
        "print(lsa_summary(original_text))\n",
        "print(\"\\nTOPSIS Weighted Sums:\")\n",
        "print(weighted_sums)\n",
        "print(\"\\nBest Model:\")\n",
        "print(best_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "p64dfnHNWmpe",
        "outputId": "438b8a33-ffe4-48b1-d4dc-895b63b3324a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim.summarization'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c70da995b4c0>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummarize\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgensim_summarize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bert-extractive-summarizer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHymqcWcXL3Y",
        "outputId": "19ba34af-f2fb-4664-fc15-3ba070c65399"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert-extractive-summarizer\n",
            "  Downloading bert_extractive_summarizer-0.10.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from bert-extractive-summarizer) (4.35.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from bert-extractive-summarizer) (1.2.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from bert-extractive-summarizer) (3.6.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->bert-extractive-summarizer) (3.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (1.10.14)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->bert-extractive-summarizer) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->bert-extractive-summarizer) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers->bert-extractive-summarizer) (0.20.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->bert-extractive-summarizer) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->bert-extractive-summarizer) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->bert-extractive-summarizer) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->bert-extractive-summarizer) (0.4.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers->bert-extractive-summarizer) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers->bert-extractive-summarizer) (4.5.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy->bert-extractive-summarizer) (0.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->bert-extractive-summarizer) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->bert-extractive-summarizer) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy->bert-extractive-summarizer) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy->bert-extractive-summarizer) (2.1.4)\n",
            "Installing collected packages: bert-extractive-summarizer\n",
            "Successfully installed bert-extractive-summarizer-0.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from gensim.summarization import summarize as gensim_summarize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from summarizer import Summarizer\n",
        "import numpy as np\n",
        "\n",
        "# Sample text\n",
        "original_text = \"\"\"\n",
        "The advancements in artificial intelligence have led to significant breakthroughs in various domains.\n",
        "Machine learning algorithms have become increasingly sophisticated, enabling computers to perform complex tasks\n",
        "with minimal human intervention. Natural Language Processing (NLP) is one such field where AI has made\n",
        "remarkable strides. NLP focuses on the interaction between computers and humans through natural language.\n",
        "This involves tasks such as language translation, sentiment analysis, and text summarization. Text summarization,\n",
        "in particular, is crucial for condensing large amounts of information into concise and informative summaries.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(original_text)\n",
        "\n",
        "# Gensim Extractive Summarization\n",
        "gensim_summary = gensim_summarize(original_text)\n",
        "\n",
        "# BERT Extractive Summarization\n",
        "bert_model = Summarizer()\n",
        "bert_summary = bert_model(original_text)\n",
        "\n",
        "# Rule-Based Summarization\n",
        "# Example: Select the first and last sentences\n",
        "rule_based_summary = sentences[0] + \" \" + sentences[-1]\n",
        "\n",
        "# Latent Semantic Analysis (LSA)\n",
        "def lsa_summary(text, num_topics=1):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform([text])\n",
        "\n",
        "    # Apply Latent Semantic Analysis\n",
        "    svd = TruncatedSVD(n_components=num_topics)\n",
        "    lsa_matrix = svd.fit_transform(X)\n",
        "\n",
        "    # Extract the most important sentence based on LSA\n",
        "    important_sentence_index = np.argmax(lsa_matrix)\n",
        "    lsa_summary = sentences[important_sentence_index]\n",
        "\n",
        "    return lsa_summary\n",
        "\n",
        "# Calculate TOPSIS scores (hypothetical)\n",
        "scores = {\n",
        "    \"gensim\": {\"Relevance\": 8, \"Coherence\": 7, \"Conciseness\": 9, \"Coverage\": 8},\n",
        "    \"bert\": {\"Relevance\": 9, \"Coherence\": 8, \"Conciseness\": 9, \"Coverage\": 9},\n",
        "    \"rule_based\": {\"Relevance\": 6, \"Coherence\": 6, \"Conciseness\": 7, \"Coverage\": 6},\n",
        "    \"lsa\": {\"Relevance\": 7, \"Coherence\": 7, \"Conciseness\": 8, \"Coverage\": 7}\n",
        "}\n",
        "\n",
        "# Weights for each criterion\n",
        "weights = {\"Relevance\": 4, \"Coherence\": 3, \"Conciseness\": 2, \"Coverage\": 1}\n",
        "\n",
        "# Calculate weighted sums\n",
        "weighted_sums = {model: sum(scores[model][criterion] * weights[criterion] for criterion in weights) for model in scores}\n",
        "\n",
        "# Determine the best model\n",
        "best_model = max(weighted_sums, key=weighted_sums.get)\n",
        "\n",
        "# Display results\n",
        "print(\"Gensim Summary:\")\n",
        "print(gensim_summary)\n",
        "print(\"\\nBERT Summary:\")\n",
        "print(bert_summary)\n",
        "print(\"\\nRule-Based Summary:\")\n",
        "print(rule_based_summary)\n",
        "print(\"\\nLSA Summary:\")\n",
        "print(lsa_summary(original_text))\n",
        "print(\"\\nTOPSIS Weighted Sums:\")\n",
        "print(weighted_sums)\n",
        "print(\"\\nBest Model:\")\n",
        "print(best_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "SowZODm_XZBZ",
        "outputId": "9032997d-c5c1-4c18-a865-ddddaec5a5e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim.summarization'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-7e0d85a4de37>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummarize\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgensim_summarize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UPRnED8XdMn",
        "outputId": "f2ee798c-a2ba-46f6-b668-c84df3a48210"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from gensim.summarization import summarize as gensim_summarize\n",
        "from summarizer import Summarizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Sample text\n",
        "original_text = \"\"\"\n",
        "The advancements in artificial intelligence have led to significant breakthroughs in various domains.\n",
        "Machine learning algorithms have become increasingly sophisticated, enabling computers to perform complex tasks\n",
        "with minimal human intervention. Natural Language Processing (NLP) is one such field where AI has made\n",
        "remarkable strides. NLP focuses on the interaction between computers and humans through natural language.\n",
        "This involves tasks such as language translation, sentiment analysis, and text summarization. Text summarization,\n",
        "in particular, is crucial for condensing large amounts of information into concise and informative summaries.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(original_text)\n",
        "\n",
        "# Gensim Extractive Summarization\n",
        "gensim_summary = gensim_summarize(original_text)\n",
        "\n",
        "# BERT Extractive Summarization\n",
        "bert_model = Summarizer()\n",
        "bert_summary = bert_model(original_text)\n",
        "\n",
        "# Rule-Based Summarization\n",
        "# Example: Select the first and last sentences\n",
        "rule_based_summary = sentences[0] + \" \" + sentences[-1]\n",
        "\n",
        "# GPT-2 Abstractive Summarization\n",
        "def gpt2_abstractive_summarization(text):\n",
        "    # Load pre-trained GPT-2 model and tokenizer\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "    # Tokenize the input text\n",
        "    input_ids = tokenizer.encode(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "    # Generate the summary using the model\n",
        "    summary_ids = model.generate(input_ids, max_length=150, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Abstractive Summarization using GPT-2\n",
        "gpt2_summary = gpt2_abstractive_summarization(original_text)\n",
        "\n",
        "# Calculate TOPSIS scores (hypothetical)\n",
        "scores = {\n",
        "    \"gensim\": {\"Relevance\": 8, \"Coherence\": 7, \"Conciseness\": 9, \"Coverage\": 8},\n",
        "    \"bert\": {\"Relevance\": 9, \"Coherence\": 8, \"Conciseness\": 9, \"Coverage\": 9},\n",
        "    \"rule_based\": {\"Relevance\": 6, \"Coherence\": 6, \"Conciseness\": 7, \"Coverage\": 6},\n",
        "    \"gpt2\": {\"Relevance\": 9, \"Coherence\": 8, \"Conciseness\": 9, \"Coverage\": 9}\n",
        "}\n",
        "\n",
        "# Weights for each criterion\n",
        "weights = {\"Relevance\": 4, \"Coherence\": 3, \"Conciseness\": 2, \"Coverage\": 1}\n",
        "\n",
        "# Calculate weighted sums\n",
        "#weighted_sums = {model: sum(scores[model][criterion] * weights[c\n",
        "# continued from the previous snippet\n",
        "\n",
        "# Calculate weighted sums\n",
        "weighted_sums = {model: sum(scores[model][criterion] * weights[criterion] for criterion in weights) for model in scores}\n",
        "\n",
        "# Determine the best model\n",
        "best_model = max(weighted_sums, key=weighted_sums.get)\n",
        "\n",
        "# Display results\n",
        "print(\"Gensim Summary:\")\n",
        "print(gensim_summary)\n",
        "print(\"\\nBERT Summary:\")\n",
        "print(bert_summary)\n",
        "print(\"\\nRule-Based Summary:\")\n",
        "print(rule_based_summary)\n",
        "print(\"\\nGPT-2 Summary:\")\n",
        "print(gpt2_summary)\n",
        "print(\"\\nTOPSIS Weighted Sums:\")\n",
        "print(weighted_sums)\n",
        "print(\"\\nBest Model:\")\n",
        "print(best_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "2dMqaVJQXyKK",
        "outputId": "3a84d3f9-3a76-44f3-d130-4bd3d9f8ce57"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim.summarization'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-63833b85dd08>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummarize\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgensim_summarize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msummarizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummarizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WKSLvGCUX0Gz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}